{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VELOCITY-ASR v2 Collaboration Notebook\n",
    "\n",
    "This notebook is a shared scratchpad for understanding the repo layout, downloading LibriSpeech, and running training. It mirrors the behavior in `scripts/download_librispeech.py`, `scripts/train.py`, and `velocity_asr/model.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repo map (fast context)\n",
    "\n",
    "- `scripts/download_librispeech.py`: download splits and optionally generate JSONL manifests\n",
    "- `scripts/train.py`: training entrypoint that reads `configs/train.yaml` and `configs/model.yaml`\n",
    "- `velocity_asr/model.py`: VELOCITY-ASR model definition\n",
    "- `velocity_asr/data.py`: dataset + dataloaders (manifest or LibriSpeech direct)\n",
    "- `configs/`: training and model hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install dependencies and the package in editable mode:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "pip install -e .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download LibriSpeech (option A: torchaudio download + manifests)\n",
    "\n",
    "This uses `scripts/download_librispeech.py`, which can also write JSONL manifests for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/download_librispeech.py --train 100 --create-manifests --data-dir ./data --manifest-dir ./manifests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect training and model configs\n",
    "\n",
    "These YAML files drive the training script and model instantiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open(\"configs/train.yaml\", \"r\") as f:\n",
    "    train_cfg = yaml.safe_load(f)\n",
    "\n",
    "with open(\"configs/model.yaml\", \"r\") as f:\n",
    "    model_cfg = yaml.safe_load(f)\n",
    "\n",
    "train_cfg, model_cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the model (from `velocity_asr/model.py`)\n",
    "\n",
    "This matches the logic in `scripts/train.py` where `VelocityASRConfig` is assembled from `configs/model.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from velocity_asr import VELOCITYASR, VelocityASRConfig\n",
    "\n",
    "cfg = VelocityASRConfig(\n",
    "    mel_bins=model_cfg.get(\"input\", {}).get(\"mel_bins\", 80),\n",
    "    d_model=model_cfg.get(\"model\", {}).get(\"d_model\", 192),\n",
    "    ssm_layers=model_cfg.get(\"ssm\", {}).get(\"num_layers\", 8),\n",
    "    ssm_state_dim=model_cfg.get(\"ssm\", {}).get(\"state_dim\", 64),\n",
    "    ssm_expand_ratio=model_cfg.get(\"ssm\", {}).get(\"expand_ratio\", 2),\n",
    "    ssm_kernel_size=model_cfg.get(\"ssm\", {}).get(\"kernel_size\", 4),\n",
    "    global_ssm_layers=model_cfg.get(\"global_context\", {}).get(\"ssm_layers\", 2),\n",
    "    global_ssm_state_dim=model_cfg.get(\"global_context\", {}).get(\"ssm_state_dim\", 32),\n",
    "    attention_heads=model_cfg.get(\"global_context\", {}).get(\"attention_heads\", 4),\n",
    "    attention_dim=model_cfg.get(\"global_context\", {}).get(\"attention_dim\", 48),\n",
    "    vocab_size=model_cfg.get(\"model\", {}).get(\"vocab_size\", 1000),\n",
    "    dropout=model_cfg.get(\"model\", {}).get(\"dropout\", 0.1),\n",
    "    gradient_checkpointing=model_cfg.get(\"memory\", {}).get(\"gradient_checkpointing\", False),\n",
    "    scan_mode=model_cfg.get(\"performance\", {}).get(\"scan_mode\", \"parallel\"),\n",
    "    use_compile=model_cfg.get(\"performance\", {}).get(\"use_compile\", False),\n",
    ")\n",
    "\n",
    "model = VELOCITYASR(cfg)\n",
    "print(f\"Params: {model.count_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training (runs `scripts/train.py`)\n",
    "\n",
    "The training script can load either JSONL manifests or LibriSpeech directly. Update `configs/train.yaml` to set `data.train_manifest`/`data.val_manifest`, or set `data.librispeech_root` for direct loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/train.py --config configs/train.yaml --model-config configs/model.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick forward pass + greedy decode (dummy audio)\n",
    "\n",
    "This is only to validate the model wiring. Output is random without trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from velocity_asr import create_default_vocabulary, CTCDecoder\n",
    "\n",
    "# Fake mel spectrogram: (batch, frames, mel_bins)\n",
    "mel = torch.randn(1, 300, cfg.mel_bins)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(mel)\n",
    "\n",
    "vocab = create_default_vocabulary(cfg.vocab_size)\n",
    "decoder = CTCDecoder(vocab)\n",
    "decoder.decode_greedy(logits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
